# -*- coding: utf-8 -*-
"""CS_210_Project_Predicting_Stock_Market_Volatility_using_Machine_Learning_Techniques_.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tPl0zriDlIibuh23Xybr04lDEmgGBkQK

# **Predicting Stock Market Volatility using Machine Learning Techniques**



The dispersion of returns for a certain stock or market index is measured by stock market volatility. The stock market's volatility can serve as a predictor of both positive and negative trends. Fund managers, analysts, and investors are all quite interested in predicting these trends to base their investment decisions upon. However, as financial markets are complicated, predicting stock market volatility is a difficult undertaking, hence why using Machine Learning Methods can possibly prove to be a handy tool for determining stock trends.



# Dataset:  
https://www.kaggle.com/datasets/khushipitroda/stock-market-historical-data-of-top-10-companies


File Name:    (  data.csv  )

# **Research Questions**

(1) How does the distribution of trading volume and stock prices (Close/Last, Open, High, Low) vary across the top 10 companies in the stock market, and how do these variables correlate with each other?

(2) Which Machine Learning Model Can be best implemented to predict a stock's closing price?  

(3) Can we classify whether a stock's closing price will increase or decrease the next day based on the current day's high, low, open, close, and volume?

(4) Do certain companies show a more consistent stock price pattern than others?
"""

# Research Question 1

# Importing required libraries
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import zscore

# Loading the dataset
#data = pd.read_csv("/Users/humzagohar/Desktop/data.csv")
data = pd.read_csv("data.csv")

# Converting Date to datetime format
data['Date'] = pd.to_datetime(data['Date'])

# Cleaning the Close/Last, Open, High, Low columns to remove '$' and convert to numeric
for col in ['Close/Last', 'Open', 'High', 'Low']:
    data[col] = pd.to_numeric(data[col].str.replace('$', ''))

# Displaying the first few rows of the dataset
print(data.head())

# Calculating variability metrics for each company
metrics = ['Volume', 'Close/Last', 'Open', 'High', 'Low']
variability = data.groupby('Company')[metrics].agg(['min', 'max', 'var', 'std'])
print(variability)

# Boxplots for trading volume and stock prices
plt.figure(figsize=(15, 10))
for i, metric in enumerate(metrics):
    plt.subplot(2, 3, i+1)
    sns.boxplot(x='Company', y=metric, data=data)
    plt.title(f'Boxplot of {metric} by Company')
plt.tight_layout()
plt.show()

# Histograms and density estimates for trading volume
plt.figure(figsize=(15, 5))
plt.subplot(1, 2, 1)
sns.histplot(data['Volume'], kde=True)
plt.title('Histogram of Trading Volume')
plt.subplot(1, 2, 2)
sns.kdeplot(data['Volume'])
plt.title('Density Estimate of Trading Volume')
plt.show()

# Scatter plot and correlation matrix
plt.figure(figsize=(10, 8))
sns.pairplot(data[metrics])
plt.show()

corr_matrix = data[metrics].corr()
print("Correlation Matrix:", corr_matrix)

# Hexagonal binning and contours for Volume vs. Close/Last
plt.figure(figsize=(15, 5))
plt.subplot(1, 2, 1)
plt.hexbin(data['Volume'], data['Close/Last'], gridsize=30, cmap='Blues')
plt.colorbar()
plt.title('Hexagonal Binning of Volume vs. Close/Last')
plt.subplot(1, 2, 2)
sns.kdeplot(data=data, x='Volume', y='Close/Last', cmap='Blues', fill=True)
plt.title('Contour Plot of Volume vs. Close/Last')
plt.show()

# Grouped boxplots for Volume and stock prices
plt.figure(figsize=(15, 10))
for i, metric in enumerate(metrics):
    plt.subplot(2, 3, i+1)
    sns.boxplot(x='Company', y=metric, data=data)
    plt.title(f'Boxplot of {metric} by Company')
plt.tight_layout()
plt.show()

# Grouped scatter plots for Volume vs. Close/Last
plt.figure(figsize=(15, 5))
sns.scatterplot(x='Volume', y='Close/Last', hue='Company', data=data)
plt.title('Scatter Plot of Volume vs. Close/Last by Company')
plt.show()

# Research Question 2

import pandas as pd

# Loading the dataset
# data = pd.read_csv("/Users/humzagohar/Desktop/data.csv")
data = pd.read_csv("data.csv")

# Converting Date to datetime format
data['Date'] = pd.to_datetime(data['Date'])

# Cleaning the Close/Last, Open, High, Low columns to remove '$' and convert to numeric
for col in ['Close/Last', 'Open', 'High', 'Low']:
    data[col] = pd.to_numeric(data[col].str.replace('$', ''))

# Creating a one-hot encoding of the Company column
data = pd.get_dummies(data, columns=['Company'], drop_first=True)

# Displaying the first few rows of the dataset
print(data.head())

from sklearn.decomposition import PCA

# Defining features and target variable
X = data.drop(columns=['Close/Last', 'Date'])
y = data['Close/Last']

# Applying PCA
pca = PCA(n_components=5)
X_pca = pca.fit_transform(X)

print("Explained variance ratios:", pca.explained_variance_ratio_)


from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)


from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_squared_error

# Random Forest
rf = RandomForestRegressor(n_estimators=100)
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)
print("Random Forest MSE:", mean_squared_error(y_test, y_pred_rf))

# Decision Tree
dt = DecisionTreeRegressor()
dt.fit(X_train, y_train)
y_pred_dt = dt.predict(X_test)
print("Decision Tree MSE:", mean_squared_error(y_test, y_pred_dt))

# KNN
knn = KNeighborsRegressor(n_neighbors=5)
knn.fit(X_train, y_train)
y_pred_knn = knn.predict(X_test)
print("KNN MSE:", mean_squared_error(y_test, y_pred_knn))

# Research Question 3

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score

# Loading the data
# data = pd.read_csv("/Users/humzagohar/Desktop/data.csv")
data = pd.read_csv("data.csv")

# Removing dollar signs and convert to float
columns_to_convert = ['Close/Last', 'Open', 'High', 'Low']
for col in columns_to_convert:
    data[col] = data[col].str.replace('$', '').astype(float)

# Creating binary variable indicating if the next day's close is higher than the current day's
data['Price_Up'] = data['Close/Last'].shift(-1) < data['Close/Last']

# Dropping the company column (for simplicity in this example)
data = data.drop(['Company', 'Date'], axis=1)

# Splitting data into train and test sets
X = data.drop(['Price_Up'], axis=1)[:-1]  # Exclude last row as it does not have a label
y = data['Price_Up'][:-1]  # Exclude last row as it does not have a label

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardizing the features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Research Question 3

# Training a logistic regression model
log_reg = LogisticRegression()
log_reg.fit(X_train, y_train)

# Predicting and evaluating
log_reg_preds = log_reg.predict(X_test)
print("Logistic Regression Results:")
print(classification_report(y_test, log_reg_preds))
print("Logistic Regression Average Accuracy:", accuracy_score(y_test, log_reg_preds))

# Displaying coefficients
coeff = pd.Series(log_reg.coef_[0], index=X.columns)
print("Logistic Regression Coefficients:")
print(coeff)

# Research Question 3

# Training a random forest classifier
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)

# Predicting and evaluating
rf_preds = rf.predict(X_test)
print("Random Forest Results:")
print(classification_report(y_test, rf_preds))
print("Random Forest Average Accuracy:", accuracy_score(y_test, rf_preds))

# Displaying feature importance
importance = pd.Series(rf.feature_importances_, index=X.columns)
print("Random Forest Feature Importance:")
print(importance)

# Research Question 4

import pandas as pd
import statsmodels.api as sm

# Loading the data
# data = pd.read_csv("/Users/humzagohar/Desktop/data.csv")
data = pd.read_csv("data.csv")

# Cleaning the data
data['Open'] = data['Open'].str.replace('$', '').astype(float)
data['Close/Last'] = data['Close/Last'].str.replace('$', '').astype(float)
data['Volume'] = data['Volume'].astype(float)

# Storing results
results_summary = {}

# Looping through each company and perform regression
for company in data['Company'].unique():
    company_data = data[data['Company'] == company]

    # Regression
    X = company_data['Volume']
    y = company_data['Close/Last']
    X = sm.add_constant(X)  # Adds a constant term to the predictor
    model = sm.OLS(y, X)
    results = model.fit()

    # Storing the results
    results_summary[company] = {
        'const_coef': results.params['const'],
        'volume_coef': results.params['Volume'],
        'const_ci': results.conf_int().loc['const'].tolist(),
        'volume_ci': results.conf_int().loc['Volume'].tolist(),
        'R-squared': results.rsquared
    }

# Displaying the results
results_df = pd.DataFrame(results_summary).T
print(results_df)